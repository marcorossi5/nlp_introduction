{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document classification with Word2Vec model\n",
    "\n",
    "## Word Embeddings\n",
    "\n",
    "The objective is to map words to real vectors. The vectors can be generated with neural networks,\n",
    "co-occurrence matrix, probabilistic models, etc.\n",
    "\n",
    "The idea is that the BoW approach leads to highly dimensional and sparse vectors,\n",
    "plus completely independent tokens. Namely, each n-gram representation does not\n",
    "change if used in different sentences.\n",
    "\n",
    "[Word2Vec](https://code.google.com/archive/p/word2vec/) is a method of building\n",
    "word embeddings with the following two strategies. The resulting vectors are\n",
    "continuous and context dependent.\n",
    "\n",
    "- Skip Gram: NN that predicts the context, given the current word.\n",
    "- Common Bag of Words (CBOW): NN that predicts the current word, given its context.\n",
    "\n",
    "In this notebook we train a Word2Vec model taken from the\n",
    "[gensim](https://radimrehurek.com/gensim/index.html) package and use it to\n",
    "perform token embedding.  \n",
    "The high dimensional embeddings are then classfied by a support vector machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\"alt.atheism\", \"soc.religion.christian\", \"comp.graphics\", \"sci.med\"]\n",
    "twenty_train = fetch_20newsgroups(\n",
    "    subset=\"train\",\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    ")\n",
    "\n",
    "twenty_test = fetch_20newsgroups(\n",
    "    subset=\"test\",\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize dataset\n",
    "import tqdm\n",
    "import gensim\n",
    "\n",
    "tokenized_train = [gensim.utils.simple_preprocess(doc) for doc in twenty_train.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec model\n",
    "# sg = 0 -> CBOW, sg = 1 skip-gram\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences=tokenized_train, vector_size=200, window=5, min_count=5, sg=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 200)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# extract a single embedding for each doc\n",
    "# use the average of its tokens vectors\n",
    "def document_vector(doc, model):\n",
    "    vecs = [\n",
    "        model.wv[token]\n",
    "        for token in gensim.utils.simple_preprocess(doc)\n",
    "        if token in model.wv\n",
    "    ]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
    "\n",
    "\n",
    "def extract_corpus_embed_matrix(corpus, model):\n",
    "    train_vectors = [document_vector(doc, model) for doc in corpus]\n",
    "    return np.stack(train_vectors, axis=0)\n",
    "\n",
    "\n",
    "x_train = extract_corpus_embed_matrix(twenty_train.data, model)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(alpha=0.001, random_state=42, tol=None)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(alpha=0.001, random_state=42, tol=None)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(alpha=0.001, random_state=42, tol=None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", alpha=1e-3, random_state=42, tol=None)\n",
    "clf.fit(x_train, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new sklearn Pipeline block\n",
    "\n",
    "Wrap `gensim.models.Word2Vec` around an sklearn BaseEstimantor to put the node\n",
    "in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Sklearn Pipeline transformer wrapper around gensim Word2Vec model\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_size: int = 100,\n",
    "        window: int = 5,\n",
    "        min_count: int = 5,\n",
    "        sg: int = 0\n",
    "    ):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.sg = sg\n",
    "\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        :param X: the corpus to train the Word2Vec on\n",
    "        :type X: List[str]\n",
    "        \"\"\"\n",
    "        # tokenize corpus\n",
    "        tokenized_train = [gensim.utils.simple_preprocess(doc) for doc in X]\n",
    "\n",
    "        # train the model\n",
    "        self.model = gensim.models.Word2Vec(\n",
    "            sentences=tokenized_train,\n",
    "            vector_size=self.vector_size,\n",
    "            window=self.window,\n",
    "            min_count=self.min_count,\n",
    "            sg=self.sg,\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def document_vector(self, doc: str):\n",
    "        # tokenize the document and compute embeddings\n",
    "        vecs = [\n",
    "            model.wv[token]\n",
    "            for token in gensim.utils.simple_preprocess(doc)\n",
    "            if token in model.wv\n",
    "        ]\n",
    "        return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\n",
    "                \"The model should call the `fit()` method before `transform()`\"\n",
    "            )\n",
    "        train_vectors = [self.document_vector(doc) for doc in X]\n",
    "        return np.stack(train_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;word2vec&#x27;, Word2VecTransformer()),\n",
       "                (&#x27;clf&#x27;, SGDClassifier(alpha=0.001, random_state=42, tol=None))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;word2vec&#x27;, Word2VecTransformer()),\n",
       "                (&#x27;clf&#x27;, SGDClassifier(alpha=0.001, random_state=42, tol=None))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Word2VecTransformer</label><div class=\"sk-toggleable__content\"><pre>Word2VecTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(alpha=0.001, random_state=42, tol=None)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('word2vec', Word2VecTransformer()),\n",
       "                ('clf', SGDClassifier(alpha=0.001, random_state=42, tol=None))])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline(\n",
    "    [\n",
    "        (\"word2vec\", Word2VecTransformer()),\n",
    "        (\n",
    "            \"clf\",\n",
    "            SGDClassifier(\n",
    "                loss=\"hinge\", penalty=\"l2\", alpha=1e-3, random_state=42, tol=None\n",
    "            ),\n",
    "        ),\n",
    "\n",
    "    ]\n",
    ")\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6830892143808256"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the pipeline\n",
    "preds = text_clf.predict(twenty_test.data)\n",
    "np.mean(preds == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"word2vec__vector_size\": [100,200,300,400,500],\n",
    "    \"word2vec__sg\": [0,1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=6)\n",
    "\n",
    "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec__sg: 0\n",
      "word2vec__vector_size: 100\n"
     ]
    }
   ],
   "source": [
    "for p in sorted(parameters.keys()):\n",
    "    print(f\"{p}: {gs_clf.best_params_[p]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_word2vec__sg</th>\n",
       "      <th>param_word2vec__vector_size</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.438784</td>\n",
       "      <td>0.089807</td>\n",
       "      <td>0.151731</td>\n",
       "      <td>0.037054</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>{'word2vec__sg': 0, 'word2vec__vector_size': 100}</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.055565</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.454347</td>\n",
       "      <td>0.166459</td>\n",
       "      <td>0.146495</td>\n",
       "      <td>0.022484</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>{'word2vec__sg': 0, 'word2vec__vector_size': 200}</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.055565</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.646499</td>\n",
       "      <td>0.099795</td>\n",
       "      <td>0.143623</td>\n",
       "      <td>0.017180</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>{'word2vec__sg': 0, 'word2vec__vector_size': 300}</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.055565</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.711491</td>\n",
       "      <td>0.103790</td>\n",
       "      <td>0.212415</td>\n",
       "      <td>0.113689</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>{'word2vec__sg': 0, 'word2vec__vector_size': 400}</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.055565</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.941890</td>\n",
       "      <td>0.309260</td>\n",
       "      <td>0.164905</td>\n",
       "      <td>0.032156</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>{'word2vec__sg': 0, 'word2vec__vector_size': 500}</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.055565</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.843275</td>\n",
       "      <td>0.105423</td>\n",
       "      <td>0.183753</td>\n",
       "      <td>0.062593</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>{'word2vec__sg': 1, 'word2vec__vector_size': 100}</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.055565</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.671470</td>\n",
       "      <td>0.243405</td>\n",
       "      <td>0.208096</td>\n",
       "      <td>0.107494</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>{'word2vec__sg': 1, 'word2vec__vector_size': 200}</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.055565</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.417891</td>\n",
       "      <td>0.126784</td>\n",
       "      <td>0.192075</td>\n",
       "      <td>0.040728</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>{'word2vec__sg': 1, 'word2vec__vector_size': 300}</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.055565</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.902230</td>\n",
       "      <td>0.155742</td>\n",
       "      <td>0.249699</td>\n",
       "      <td>0.192018</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>{'word2vec__sg': 1, 'word2vec__vector_size': 400}</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.055565</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.604108</td>\n",
       "      <td>1.025849</td>\n",
       "      <td>0.091894</td>\n",
       "      <td>0.028560</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>{'word2vec__sg': 1, 'word2vec__vector_size': 500}</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.055565</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       2.438784      0.089807         0.151731        0.037054   \n",
       "1       2.454347      0.166459         0.146495        0.022484   \n",
       "2       2.646499      0.099795         0.143623        0.017180   \n",
       "3       2.711491      0.103790         0.212415        0.113689   \n",
       "4       2.941890      0.309260         0.164905        0.032156   \n",
       "5       2.843275      0.105423         0.183753        0.062593   \n",
       "6       3.671470      0.243405         0.208096        0.107494   \n",
       "7       4.417891      0.126784         0.192075        0.040728   \n",
       "8       4.902230      0.155742         0.249699        0.192018   \n",
       "9       3.604108      1.025849         0.091894        0.028560   \n",
       "\n",
       "  param_word2vec__sg param_word2vec__vector_size  \\\n",
       "0                  0                         100   \n",
       "1                  0                         200   \n",
       "2                  0                         300   \n",
       "3                  0                         400   \n",
       "4                  0                         500   \n",
       "5                  1                         100   \n",
       "6                  1                         200   \n",
       "7                  1                         300   \n",
       "8                  1                         400   \n",
       "9                  1                         500   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'word2vec__sg': 0, 'word2vec__vector_size': 100}              0.675   \n",
       "1  {'word2vec__sg': 0, 'word2vec__vector_size': 200}              0.675   \n",
       "2  {'word2vec__sg': 0, 'word2vec__vector_size': 300}              0.675   \n",
       "3  {'word2vec__sg': 0, 'word2vec__vector_size': 400}              0.675   \n",
       "4  {'word2vec__sg': 0, 'word2vec__vector_size': 500}              0.675   \n",
       "5  {'word2vec__sg': 1, 'word2vec__vector_size': 100}              0.675   \n",
       "6  {'word2vec__sg': 1, 'word2vec__vector_size': 200}              0.675   \n",
       "7  {'word2vec__sg': 1, 'word2vec__vector_size': 300}              0.675   \n",
       "8  {'word2vec__sg': 1, 'word2vec__vector_size': 400}              0.675   \n",
       "9  {'word2vec__sg': 1, 'word2vec__vector_size': 500}              0.675   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0             0.6625               0.65                0.8             0.6625   \n",
       "1             0.6625               0.65                0.8             0.6625   \n",
       "2             0.6625               0.65                0.8             0.6625   \n",
       "3             0.6625               0.65                0.8             0.6625   \n",
       "4             0.6625               0.65                0.8             0.6625   \n",
       "5             0.6625               0.65                0.8             0.6625   \n",
       "6             0.6625               0.65                0.8             0.6625   \n",
       "7             0.6625               0.65                0.8             0.6625   \n",
       "8             0.6625               0.65                0.8             0.6625   \n",
       "9             0.6625               0.65                0.8             0.6625   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0             0.69        0.055565                1  \n",
       "1             0.69        0.055565                1  \n",
       "2             0.69        0.055565                1  \n",
       "3             0.69        0.055565                1  \n",
       "4             0.69        0.055565                1  \n",
       "5             0.69        0.055565                1  \n",
       "6             0.69        0.055565                1  \n",
       "7             0.69        0.055565                1  \n",
       "8             0.69        0.055565                1  \n",
       "9             0.69        0.055565                1  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(gs_clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
